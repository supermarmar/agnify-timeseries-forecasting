{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and libraries\n",
    "import pandas as pd\n",
    "import pandasql as psql\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import scipy.stats as stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import shapiro, jarque_bera\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "from numpy import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading datasets\n",
    "# 1. White maize\n",
    "area_white_maize = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Area\\Area Grown Maize.xlsx\",\n",
    "    sheet_name=\"Area WM (ha)\",\n",
    ")\n",
    "production_white_maize = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Production\\Production Maize.xlsx\",\n",
    "    sheet_name=\"Production WM (t)\",\n",
    ")\n",
    "area_white_maize = area_white_maize.drop(area_white_maize.columns[[1, 2]], axis=1)\n",
    "area_white_maize.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "area_white_maize.set_index(\"Season\", inplace=True)\n",
    "production_white_maize.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "production_white_maize.set_index(\"Season\", inplace=True)\n",
    "# 2. Yellow maize\n",
    "area_yellow_maize = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Area\\Area Grown Maize.xlsx\",\n",
    "    sheet_name=\"Area YM (ha)\",\n",
    ")\n",
    "production_yellow_maize = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Production\\Production Maize.xlsx\",\n",
    "    sheet_name=\"Production YM (t)\",\n",
    ")\n",
    "area_yellow_maize = area_yellow_maize.drop(area_yellow_maize.columns[[1, 2]], axis=1)\n",
    "area_yellow_maize.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "area_yellow_maize.set_index(\"Season\", inplace=True)\n",
    "production_yellow_maize.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "production_yellow_maize.set_index(\"Season\", inplace=True)\n",
    "# 3. Soyabean\n",
    "area_soyabean = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Area\\Area Grown Soyabean.xlsx\"\n",
    ")\n",
    "production_soyabean = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Production\\Production Soyabean.xlsx\"\n",
    ")\n",
    "area_soyabean = area_soyabean.drop(area_soyabean.columns[[1, 2]], axis=1)\n",
    "area_soyabean.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "area_soyabean.set_index(\"Season\", inplace=True)\n",
    "production_soyabean.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "production_soyabean.set_index(\"Season\", inplace=True)\n",
    "# 4. Sunflower\n",
    "area_sunflower = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Area\\Area Grown Sunflower.xlsx\"\n",
    ")\n",
    "production_sunflower = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Production\\Production Sunflower.xlsx\"\n",
    ")\n",
    "area_sunflower = area_sunflower.drop(area_sunflower.columns[[1, 2]], axis=1)\n",
    "area_sunflower.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "area_sunflower.set_index(\"Season\", inplace=True)\n",
    "production_sunflower.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "production_sunflower.set_index(\"Season\", inplace=True)\n",
    "# 5. Wheat\n",
    "area_wheat = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Area\\Area Grown Wheat.xlsx\"\n",
    ")\n",
    "production_wheat = pd.read_excel(\n",
    "    r\"C:\\Users\\seana\\OneDrive - Agnify (Pty) Ltd\\Ag.io\\Crop Data\\Production\\Production Wheat.xlsx\"\n",
    ")\n",
    "# area_wheat = area_wheat.drop(area_sunflower.columns[[1, 2]], axis=1)\n",
    "area_wheat.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "area_wheat.set_index(\"Season\", inplace=True)\n",
    "production_wheat.rename(columns={\"Production Region\": \"Season\"}, inplace=True)\n",
    "production_wheat.set_index(\"Season\", inplace=True)\n",
    "\n",
    "# production dictionary\n",
    "production_data = {\n",
    "    \"white maize\": pd.DataFrame(production_white_maize),\n",
    "    \"yellow maize\": pd.DataFrame(production_yellow_maize),\n",
    "    \"soyabean\": pd.DataFrame(production_soyabean),\n",
    "    \"sunflower\": pd.DataFrame(production_sunflower),\n",
    "    \"wheat\": pd.DataFrame(production_wheat),\n",
    "}\n",
    "\n",
    "# area dictionary\n",
    "area_data = {\n",
    "    \"white maize\": pd.DataFrame(area_white_maize),\n",
    "    \"yellow maize\": pd.DataFrame(area_yellow_maize),\n",
    "    \"soyabean\": pd.DataFrame(area_soyabean),\n",
    "    \"sunflower\": pd.DataFrame(area_sunflower),\n",
    "    \"wheat\": pd.DataFrame(area_wheat),\n",
    "}\n",
    "\n",
    "# Dry Land Yield Estimate directory\n",
    "dry_yield_estimates = {\n",
    "    \"white maize\": [2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7],\n",
    "    \"yellow maize\": [2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, 6.5, 7],\n",
    "    \"soyabean\": [0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5],\n",
    "    \"sunflower\": [0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5],\n",
    "    \"wheat\": [1, 1.5, 2, 2.5, 3, 3.5, 4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production Index:\n",
      " Index(['Western Cape', 'Northern Cape', 'Free State', 'Eastern Cape',\n",
      "       'Kwa Zulu Natal', 'Mpumalanga', 'Limpopo', 'Gauteng', 'North West'],\n",
      "      dtype='object', name='Season')\n",
      "Area Index:\n",
      " Index(['Western Cape', 'Northern Cape', 'Free State', 'Eastern Cape',\n",
      "       'Kwa Zulu Natal', 'Mpumalanga', 'Limpopo', 'Gauteng', 'North West'],\n",
      "      dtype='object', name='Season')\n",
      "Production Columns:\n",
      " Index(['1990/91', '1991/92', '1992/93', '1993/94', '1994/95', '1995/96',\n",
      "       '1996/97', '1997/98', '1998/99', '1999/2000', '2000/2001', '2001/2002',\n",
      "       '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007',\n",
      "       '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012',\n",
      "       '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017',\n",
      "       '2017/2018', '2018/19', '2019/20', '2020/21', '2021/22', '2022/23',\n",
      "       '2023/24*', '2024/25', '2025/26', '2026/27'],\n",
      "      dtype='object')\n",
      "Area Columns:\n",
      " Index(['1990/91', '1991/92', '1992/93', '1993/94', '1994/95', '1995/96',\n",
      "       '1996/97', '1997/98', '1998/99', '1999/2000', '2000/2001', '2001/2002',\n",
      "       '2002/2003', '2003/2004', '2004/2005', '2005/2006', '2006/2007',\n",
      "       '2007/2008', '2008/2009', '2009/2010', '2010/2011', '2011/2012',\n",
      "       '2012/2013', '2013/2014', '2014/2015', '2015/2016', '2016/2017',\n",
      "       '2017/2018', '2018/19', '2019/20', '2020/21', '2021/22', '2022/23',\n",
      "       '2023/24*', '2024/25', '2025/26', '2026/27'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>SA Average Yield</th>\n",
       "      <th>SA Weighted Average Yield</th>\n",
       "      <th>Free State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990/91</td>\n",
       "      <td>1.276556</td>\n",
       "      <td>1.448276</td>\n",
       "      <td>1.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991/92</td>\n",
       "      <td>0.733957</td>\n",
       "      <td>0.758105</td>\n",
       "      <td>0.835165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992/93</td>\n",
       "      <td>1.274811</td>\n",
       "      <td>1.491304</td>\n",
       "      <td>1.181368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993/94</td>\n",
       "      <td>0.911159</td>\n",
       "      <td>1.147273</td>\n",
       "      <td>1.083617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994/95</td>\n",
       "      <td>0.746505</td>\n",
       "      <td>0.895385</td>\n",
       "      <td>1.085609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1995/96</td>\n",
       "      <td>1.377699</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>1.470354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1996/97</td>\n",
       "      <td>1.561207</td>\n",
       "      <td>1.380282</td>\n",
       "      <td>1.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997/98</td>\n",
       "      <td>1.561560</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.424129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998/99</td>\n",
       "      <td>1.502473</td>\n",
       "      <td>1.524904</td>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1999/2000</td>\n",
       "      <td>1.802196</td>\n",
       "      <td>1.641166</td>\n",
       "      <td>1.657143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000/2001</td>\n",
       "      <td>2.966293</td>\n",
       "      <td>1.687605</td>\n",
       "      <td>1.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2001/2002</td>\n",
       "      <td>2.723936</td>\n",
       "      <td>1.797517</td>\n",
       "      <td>1.363905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2002/2003</td>\n",
       "      <td>1.668267</td>\n",
       "      <td>1.363428</td>\n",
       "      <td>1.151316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2003/2004</td>\n",
       "      <td>1.950123</td>\n",
       "      <td>1.629630</td>\n",
       "      <td>1.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2004/2005</td>\n",
       "      <td>2.243597</td>\n",
       "      <td>1.816667</td>\n",
       "      <td>1.463415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2005/2006</td>\n",
       "      <td>2.023542</td>\n",
       "      <td>1.762481</td>\n",
       "      <td>1.711111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2006/2007</td>\n",
       "      <td>1.514444</td>\n",
       "      <td>1.120219</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2007/2008</td>\n",
       "      <td>1.948518</td>\n",
       "      <td>1.704958</td>\n",
       "      <td>1.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2008/2009</td>\n",
       "      <td>2.129934</td>\n",
       "      <td>2.170347</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2009/2010</td>\n",
       "      <td>1.933543</td>\n",
       "      <td>1.817306</td>\n",
       "      <td>1.599474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2010/2011</td>\n",
       "      <td>1.857269</td>\n",
       "      <td>1.698565</td>\n",
       "      <td>1.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2011/2012</td>\n",
       "      <td>1.623917</td>\n",
       "      <td>1.377119</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2012/2013</td>\n",
       "      <td>1.722222</td>\n",
       "      <td>1.523911</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2013/2014</td>\n",
       "      <td>2.107721</td>\n",
       "      <td>1.885067</td>\n",
       "      <td>1.751244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2014/2015</td>\n",
       "      <td>2.104603</td>\n",
       "      <td>1.556817</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2015/2016</td>\n",
       "      <td>1.857488</td>\n",
       "      <td>1.475736</td>\n",
       "      <td>0.850575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016/2017</td>\n",
       "      <td>2.494444</td>\n",
       "      <td>2.293527</td>\n",
       "      <td>2.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017/2018</td>\n",
       "      <td>2.169815</td>\n",
       "      <td>1.956301</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018/19</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.602115</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019/20</td>\n",
       "      <td>2.077778</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020/21</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>2.285637</td>\n",
       "      <td>2.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021/22</td>\n",
       "      <td>2.606846</td>\n",
       "      <td>2.410029</td>\n",
       "      <td>2.198795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2022/23</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>2.399460</td>\n",
       "      <td>2.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023/24*</td>\n",
       "      <td>2.116667</td>\n",
       "      <td>1.546102</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Season  SA Average Yield  SA Weighted Average Yield  Free State\n",
       "0     1990/91          1.276556                   1.448276    1.285714\n",
       "1     1991/92          0.733957                   0.758105    0.835165\n",
       "2     1992/93          1.274811                   1.491304    1.181368\n",
       "3     1993/94          0.911159                   1.147273    1.083617\n",
       "4     1994/95          0.746505                   0.895385    1.085609\n",
       "5     1995/96          1.377699                   1.176471    1.470354\n",
       "6     1996/97          1.561207                   1.380282    1.708333\n",
       "7     1997/98          1.561560                   1.720000    1.424129\n",
       "8     1998/99          1.502473                   1.524904    1.266667\n",
       "9   1999/2000          1.802196                   1.641166    1.657143\n",
       "10  2000/2001          2.966293                   1.687605    1.365000\n",
       "11  2001/2002          2.723936                   1.797517    1.363905\n",
       "12  2002/2003          1.668267                   1.363428    1.151316\n",
       "13  2003/2004          1.950123                   1.629630    1.348837\n",
       "14  2004/2005          2.243597                   1.816667    1.463415\n",
       "15  2005/2006          2.023542                   1.762481    1.711111\n",
       "16  2006/2007          1.514444                   1.120219    0.750000\n",
       "17  2007/2008          1.948518                   1.704958    1.343750\n",
       "18  2008/2009          2.129934                   2.170347    1.800000\n",
       "19  2009/2010          1.933543                   1.817306    1.599474\n",
       "20  2010/2011          1.857269                   1.698565    1.407407\n",
       "21  2011/2012          1.623917                   1.377119    1.100000\n",
       "22  2012/2013          1.722222                   1.523911    1.050000\n",
       "23  2013/2014          2.107721                   1.885067    1.751244\n",
       "24  2014/2015          2.104603                   1.556817    1.200000\n",
       "25  2015/2016          1.857488                   1.475736    0.850575\n",
       "26  2016/2017          2.494444                   2.293527    2.100000\n",
       "27  2017/2018          2.169815                   1.956301    1.600000\n",
       "28    2018/19          1.900000                   1.602115    1.300000\n",
       "29    2019/20          2.077778                   1.766667    1.650000\n",
       "30    2020/21          2.533333                   2.285637    2.100000\n",
       "31    2021/22          2.606846                   2.410029    2.198795\n",
       "32    2022/23          2.555556                   2.399460    2.350000\n",
       "33   2023/24*          2.116667                   1.546102    1.250000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to collect data based on province, crop, and number of years (max 34)\n",
    "def data_collection(prov, crop, years):\n",
    "    # production and area data selection based on crop\n",
    "    area = area_data[crop]\n",
    "    prod = production_data[crop]\n",
    "    prod = prod.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    area = area.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    prod.fillna(0, inplace=True)\n",
    "    area.fillna(0, inplace=True)\n",
    "    yield_df = prod.div(area).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    # yield calculation\n",
    "    # yield (t/ha) calculation with function for error handling\n",
    "    def yield_cal(prod, area):\n",
    "        if prod == 0 or area == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return prod / area\n",
    "\n",
    "    # Apply the function element-wise to the sunflower DataFrame\n",
    "    # yield_df = prod.applymap(lambda x: float(x)) / area.applymap(lambda x: float(x))\n",
    "    # yield_df = yield_df.applymap(lambda x: 0 if pd.isna(x) or x == float('inf') else x)\n",
    "    # reset production region as column again\n",
    "    yield_df.reset_index(inplace=True)\n",
    "\n",
    "    # Calculate the average yield per season across all regions\n",
    "    average_yield_per_season = yield_df.mean(axis=0)\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    average_yield_per_season_df = average_yield_per_season.reset_index()\n",
    "    average_yield_per_season_df.columns = [\"Season\", \"SA Average Yield\"]\n",
    "\n",
    "    # Calculate total production and total area per season\n",
    "    total_production_per_season = prod.sum(axis=0)\n",
    "    total_area_per_season = area.sum(axis=0)\n",
    "\n",
    "    # Calculate the weighted average yield per season\n",
    "    weighted_average_yield_per_season = (\n",
    "        total_production_per_season / total_area_per_season\n",
    "    )\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    weighted_average_yield_per_season_df = (\n",
    "        weighted_average_yield_per_season.reset_index()\n",
    "    )\n",
    "    weighted_average_yield_per_season_df.columns = [\n",
    "        \"Season\",\n",
    "        \"SA Weighted Average Yield\",\n",
    "    ]\n",
    "    all_seasons = weighted_average_yield_per_season_df[\"Season\"][:years]\n",
    "    if years <= len(all_seasons):\n",
    "        seasons = all_seasons[-years:]\n",
    "    else:\n",
    "        raise ValueError(\"The number of years exceeds the available seasons.\")\n",
    "\n",
    "    # Merge the average yield and weighted average yield DataFrames\n",
    "    combined_yield_df = pd.merge(\n",
    "        average_yield_per_season_df, weighted_average_yield_per_season_df, on=\"Season\"\n",
    "    )\n",
    "\n",
    "    # Select the relevant seasons\n",
    "    combined_yield_df = combined_yield_df[combined_yield_df[\"Season\"].isin(seasons)]\n",
    "\n",
    "    # Select the yields for the given province\n",
    "    reg_yields = yield_df[yield_df[\"Season\"] == prov][seasons]\n",
    "    lr_data = reg_yields.transpose()\n",
    "    lr_data.reset_index(inplace=True)\n",
    "    lr_data.columns = [\"Season\", prov]\n",
    "\n",
    "    # Merge the province yields with the combined yield DataFrame\n",
    "    yield_df = pd.merge(combined_yield_df, lr_data, on=\"Season\")\n",
    "    print(\"Production Index:\\n\", prod.index)\n",
    "    print(\"Area Index:\\n\", area.index)\n",
    "    print(\"Production Columns:\\n\", prod.columns)\n",
    "    print(\"Area Columns:\\n\", area.columns)\n",
    "    return yield_df\n",
    "\n",
    "\n",
    "soya = data_collection(\"Free State\", \"soyabean\", 34)\n",
    "soya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining all functions\n",
    "\n",
    "\n",
    "# yield (t/ha) calculation with function for error handling\n",
    "def yield_cal(prod, area):\n",
    "    if prod == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return prod / area\n",
    "\n",
    "\n",
    "# function to collect data based on province, crop, and number of years (max 34)\n",
    "def data_collection(prov, crop, years):\n",
    "    # production and area data selection based on crop\n",
    "    area = area_data[crop]\n",
    "    prod = production_data[crop]\n",
    "    # yield calculation\n",
    "    # Apply the function element-wise to the sunflower DataFrame\n",
    "    yield_df = prod.applymap(lambda x: float(x)) / area.applymap(lambda x: float(x))\n",
    "    yield_df = yield_df.applymap(lambda x: 0 if pd.isna(x) or x == float(\"inf\") else x)\n",
    "    # reset production region as column again\n",
    "    yield_df.reset_index(inplace=True)\n",
    "\n",
    "    # Calculate the average yield per season across all regions\n",
    "    average_yield_per_season = yield_df.mean(axis=0)\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    average_yield_per_season_df = average_yield_per_season.reset_index()\n",
    "    average_yield_per_season_df.columns = [\"Season\", \"SA Average Yield\"]\n",
    "\n",
    "    # Calculate total production and total area per season\n",
    "    total_production_per_season = prod.sum(axis=0)\n",
    "    total_area_per_season = area.sum(axis=0)\n",
    "\n",
    "    # Calculate the weighted average yield per season\n",
    "    weighted_average_yield_per_season = (\n",
    "        total_production_per_season / total_area_per_season\n",
    "    )\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    weighted_average_yield_per_season_df = (\n",
    "        weighted_average_yield_per_season.reset_index()\n",
    "    )\n",
    "    weighted_average_yield_per_season_df.columns = [\n",
    "        \"Season\",\n",
    "        \"SA Weighted Average Yield\",\n",
    "    ]\n",
    "    all_seasons = weighted_average_yield_per_season_df[\"Season\"][:years]\n",
    "    if years <= len(all_seasons):\n",
    "        seasons = all_seasons[-years:]\n",
    "    else:\n",
    "        raise ValueError(\"The number of years exceeds the available seasons.\")\n",
    "\n",
    "    # Merge the average yield and weighted average yield DataFrames\n",
    "    combined_yield_df = pd.merge(\n",
    "        average_yield_per_season_df, weighted_average_yield_per_season_df, on=\"Season\"\n",
    "    )\n",
    "\n",
    "    # Select the relevant seasons\n",
    "    combined_yield_df = combined_yield_df[combined_yield_df[\"Season\"].isin(seasons)]\n",
    "\n",
    "    # Select the yields for the given province\n",
    "    reg_yields = yield_df[yield_df[\"Season\"] == prov][seasons]\n",
    "    lr_data = reg_yields.transpose()\n",
    "    lr_data.reset_index(inplace=True)\n",
    "    lr_data.columns = [\"Season\", prov]\n",
    "\n",
    "    # Merge the province yields with the combined yield DataFrame\n",
    "    yield_df = pd.merge(combined_yield_df, lr_data, on=\"Season\")\n",
    "    return yield_df\n",
    "\n",
    "\n",
    "# Define function to perform regression analysis on data collected\n",
    "def yield_regression_analysis(prov, crop, years):\n",
    "    yield_data = data_collection(prov, crop, years)\n",
    "    seasons = yield_data[\"Season\"]\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    result = {\"Data Level\": [], \"Slope\": [], \"Intercept\": [], \"Predicted Value\": []}\n",
    "\n",
    "    def perform_regression(data, period):\n",
    "        X = np.arange(period).reshape(-1, 1)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, data)\n",
    "        slope = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        predicted_value = model.predict([[period]])[0]\n",
    "        return slope, intercept, predicted_value\n",
    "\n",
    "    columns = yield_data.columns.drop(\"Season\")\n",
    "\n",
    "    for column in columns:\n",
    "        data = yield_data[column].values\n",
    "        years = len(data)\n",
    "        slope, intercept, predicted_value = perform_regression(data, years)\n",
    "        result[\"Data Level\"].append(column)\n",
    "        result[\"Slope\"].append(slope)\n",
    "        result[\"Intercept\"].append(intercept)\n",
    "        result[\"Predicted Value\"].append(predicted_value)\n",
    "\n",
    "        # Plot the trends (optional if want to display results in graph)\n",
    "        # plt.figure()\n",
    "        # plt.plot(range(years), data, 'o', label='Actual Data')\n",
    "        # plt.plot(range(years), intercept + slope * np.arange(years), '-', label='Trend Line')\n",
    "        # plt.xlabel('Year')\n",
    "        # plt.ylabel(column)\n",
    "        # plt.title(f'Trend for {column}')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# Test for stationarity using AD-Fuller test\n",
    "def adf_test(prov, crop, years):\n",
    "    data = data_collection(prov, crop, years)\n",
    "    # AD-Fuller test\n",
    "    adft = adfuller(data[prov], autolag=\"AIC\")\n",
    "    output_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Metric\": [\n",
    "                \"Test Statistics\",\n",
    "                \"p-value\",\n",
    "                \"No. of lags used\",\n",
    "                \"Number of observations used\",\n",
    "                \"critical value (1%)\",\n",
    "                \"critical value (5%)\",\n",
    "                \"critical value (10%)\",\n",
    "            ],\n",
    "            \"Values\": [\n",
    "                adft[0],\n",
    "                adft[1],\n",
    "                adft[2],\n",
    "                adft[3],\n",
    "                adft[4][\"1%\"],\n",
    "                adft[4][\"5%\"],\n",
    "                adft[4][\"10%\"],\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Test H0:Series has a unit root (Non-stationary) vs H1: Series has no unit root (Stationary)\n",
    "    p_value = output_df[\"Values\"][output_df[\"Metric\"] == \"p-value\"].values[0]\n",
    "\n",
    "    if p_value < 0.01:\n",
    "        print(\n",
    "            f\"Since the p-value = {p_value} is smaller than 0.01, this means we reject the null hypothesis and, hence, the series is stationary at a 99% confidence level.\"\n",
    "        )\n",
    "    elif p_value < 0.05:\n",
    "        print(\n",
    "            f\"Since the p-value = {p_value} is smaller than 0.05, this means we reject the null hypothesis and, hence, the series is stationary at a 95% confidence level.\"\n",
    "        )\n",
    "    elif p_value < 0.1:\n",
    "        print(\n",
    "            f\"Since the p-value = {p_value} is smaller than 0.1, this means we reject the null hypothesis and, hence, the series is stationary at a 90% confidence level.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Since the p-value = {p_value} is greater than 0.1, as well as all other significant levels, this means we fail to reject the null hypothesis. Hence, the ADF test suggests that the time series is non-stationary and likely has a unit root, meaning further differencing and transformation is required.\"\n",
    "        )\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_fitting(prov, crop, years, training_size):\n",
    "    collection = data_collection(prov, crop, years)\n",
    "    collection[\"Season\"] = collection.index\n",
    "    train_size = int(len(collection) * training_size)\n",
    "    train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "    model = auto_arima(\n",
    "        train[prov],\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "    )\n",
    "    # Forecasting the test data range\n",
    "    n_periods = len(test)\n",
    "    forecast, conf_int = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "\n",
    "    # Convert forecasts to a DataFrame for better visualization and understanding\n",
    "    forecast_index = (\n",
    "        test.index\n",
    "    )  # Assuming you want to forecast for the same indices as the test set\n",
    "    forecast_df = pd.DataFrame(\n",
    "        {\"Forecast\": forecast, \"Lower CI\": conf_int[:, 0], \"Upper CI\": conf_int[:, 1]},\n",
    "        index=forecast_index,\n",
    "    )\n",
    "\n",
    "    print(forecast_df)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train.index, train[prov], label=\"Training Data\")\n",
    "    plt.plot(test.index, test[prov], label=\"Actual Data\", color=\"green\")\n",
    "    plt.plot(\n",
    "        forecast_df.index, forecast_df[\"Forecast\"], label=\"Forecasted Data\", color=\"red\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"Lower CI\"],\n",
    "        forecast_df[\"Upper CI\"],\n",
    "        color=\"orange\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    plt.title(\"Actual vs Forecasted Yields\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_fitting_accuracy(prov, crop, years, training_size):\n",
    "    collection = data_collection(prov, crop, years)\n",
    "    collection[\"Season\"] = collection.index\n",
    "    train_size = int(len(collection) * training_size)\n",
    "    train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "    model = auto_arima(\n",
    "        train[prov],\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "    )\n",
    "    # Forecasting the test data range\n",
    "    n_periods = len(test)\n",
    "    forecast, conf_int = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "\n",
    "    # Convert forecasts to a DataFrame for better visualization and understanding\n",
    "    forecast_index = (\n",
    "        test.index\n",
    "    )  # Assuming you want to forecast for the same indices as the test set\n",
    "    forecast_df = pd.DataFrame(\n",
    "        {\"Forecast\": forecast, \"Lower CI\": conf_int[:, 0], \"Upper CI\": conf_int[:, 1]},\n",
    "        index=forecast_index,\n",
    "    )\n",
    "\n",
    "    print(forecast_df)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train.index, train[prov], label=\"Training Data\")\n",
    "    plt.plot(test.index, test[prov], label=\"Actual Data\", color=\"green\")\n",
    "    plt.plot(\n",
    "        forecast_df.index, forecast_df[\"Forecast\"], label=\"Forecasted Data\", color=\"red\"\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        forecast_df.index,\n",
    "        forecast_df[\"Lower CI\"],\n",
    "        forecast_df[\"Upper CI\"],\n",
    "        color=\"orange\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    plt.title(\"Actual vs Forecasted Yields\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Calculate MAE, MSE, RMSE, and MAPE\n",
    "    mae = mean_absolute_error(test[prov], forecast)\n",
    "    mse = mean_squared_error(test[prov], forecast)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((test[prov] - forecast) / test[prov])) * 100\n",
    "\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {mape}%\")\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_residual_plots(prov, crop, years, training_size):\n",
    "    collection = data_collection(prov, crop, years)\n",
    "    collection[\"Season\"] = collection.index\n",
    "    train_size = int(len(collection) * training_size)\n",
    "    train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "    model = auto_arima(\n",
    "        train[prov],\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "    )\n",
    "    # Get residuals from the ARIMA model\n",
    "    residuals = model.resid()\n",
    "    # Plot the residuals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(residuals)\n",
    "    plt.title(\"Residuals of ARIMA Model\")\n",
    "    plt.show()\n",
    "    # Histogram of residuals\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(residuals, bins=20, edgecolor=\"k\")\n",
    "    plt.title(\"Histogram of Residuals\")\n",
    "    plt.show()\n",
    "    # Q-Q plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    plt.show()\n",
    "    # Plot ACF and PACF of residuals\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_acf(residuals, lags=20)\n",
    "    plt.title(\"Autocorrelation Function of Residuals\")\n",
    "    plt.show()\n",
    "    plot_pacf(residuals, lags=10)\n",
    "    plt.title(\"Partial Autocorrelation Function of Residuals\")\n",
    "    plt.show()\n",
    "    # Perform Ljung-Box test\n",
    "    ljung_box_results = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "    lb_stat, lb_pvalue = ljung_box_results\n",
    "    print(f\"Ljung-Box test test statistic: {ljung_box_results[lb_stat]}\")\n",
    "    print(f\"Ljung-Box test P value: {ljung_box_results[lb_pvalue]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for logic for selecting and returning the best-fitting model\n",
    "# based on AIC, BIC, and log likelihood. The function will also return the summary of the best model.\n",
    "\n",
    "\n",
    "def fitting_residuals(prov, crop, years, training_size):\n",
    "    # Data collection based on inputs\n",
    "    collection = data_collection(prov, crop, years)\n",
    "    # Resetting index\n",
    "    collection[\"Season\"] = collection.index\n",
    "    # Defining training and test data\n",
    "    train_size = int(len(collection) * training_size)\n",
    "    train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "\n",
    "    # Fitting the ARIMA model to the data\n",
    "    model = auto_arima(\n",
    "        train[prov],\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "    )\n",
    "\n",
    "    # Get residuals from the ARIMA model\n",
    "    residuals = model.resid()\n",
    "\n",
    "    # Fit GARCH model to the residuals\n",
    "    garch_model = arch_model(residuals, vol=\"Garch\", p=1, q=1)\n",
    "    garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "    # Fit EGARCH model\n",
    "    egarch_model = arch_model(residuals, vol=\"EGARCH\", p=1, q=1, dist=\"Normal\")\n",
    "    egarch_fit = egarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Fit TGARCH model\n",
    "    tgarch_model = arch_model(\n",
    "        residuals, vol=\"Garch\", p=1, q=1, dist=\"Normal\", power=1.0, o=1\n",
    "    )\n",
    "    tgarch_fit = tgarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Comparing models based on AIC, BIC, and log likelihood\n",
    "    models = {\"GARCH\": garch_fit, \"EGARCH\": egarch_fit, \"TGARCH\": tgarch_fit}\n",
    "\n",
    "    best_model = min(\n",
    "        models.values(), key=lambda fit: (fit.aic, fit.bic, -fit.loglikelihood)\n",
    "    )\n",
    "    # Extract parameters from fitted model\n",
    "    mu_garch = best_model.params[\"mu\"]\n",
    "    omega_garch = best_model.params[\"omega\"]\n",
    "    alpha_garch = best_model.params[\"alpha[1]\"]\n",
    "    beta_garch = best_model.params[\"beta[1]\"]\n",
    "\n",
    "    # Print and return the best model\n",
    "    print(\"\\nBest model for residuals based on AIC, BIC, and log likelihood:\")\n",
    "    print(best_model.model.volatility.__class__.__name__)\n",
    "    print(best_model.summary())\n",
    "    print(f\"mu: {mu_garch}\")\n",
    "    print(f\"omega: {omega_garch}\")\n",
    "    print(f\"alpha[1]: {alpha_garch}\")\n",
    "    print(f\"beta[1]: {beta_garch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_best_fit_parameters(prov, crop, years, training_size):\n",
    "    # Data collection based on inputs\n",
    "    collection = data_collection(prov, crop, years)\n",
    "    # Resetting index\n",
    "    collection[\"Season\"] = collection.index\n",
    "    # Defining training and test data\n",
    "    train_size = int(len(collection) * training_size)\n",
    "    train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "\n",
    "    # Fitting the ARIMA model to the data\n",
    "    model = auto_arima(\n",
    "        train[prov],\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "    )\n",
    "\n",
    "    # Get residuals from the ARIMA model\n",
    "    residuals = model.resid()\n",
    "\n",
    "    # Fit GARCH model to the residuals\n",
    "    garch_model = arch_model(residuals, vol=\"Garch\", p=1, q=1)\n",
    "    garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "    # Fit EGARCH model\n",
    "    egarch_model = arch_model(residuals, vol=\"EGARCH\", p=1, q=1, dist=\"Normal\")\n",
    "    egarch_fit = egarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Fit TGARCH model\n",
    "    tgarch_model = arch_model(\n",
    "        residuals, vol=\"Garch\", p=1, q=1, dist=\"Normal\", power=1.0, o=1\n",
    "    )\n",
    "    tgarch_fit = tgarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Comparing models based on AIC, BIC, and log likelihood\n",
    "    models = {\"GARCH\": garch_fit, \"EGARCH\": egarch_fit, \"TGARCH\": tgarch_fit}\n",
    "\n",
    "    best_model = min(\n",
    "        models.values(), key=lambda fit: (fit.aic, fit.bic, -fit.loglikelihood)\n",
    "    )\n",
    "    # Extract parameters from fitted model\n",
    "    mu_garch = best_model.params[\"mu\"]\n",
    "    omega_garch = best_model.params[\"omega\"]\n",
    "    alpha_garch = best_model.params[\"alpha[1]\"]\n",
    "    beta_garch = best_model.params[\"beta[1]\"]\n",
    "\n",
    "    print(\n",
    "        f\"\\nMu (μ): This parameter represents the constant term in the mean equation. Here, μ={round(mu_garch, 2)}. This is the average level of the residuals when accounting for past information.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nOmega (ω): This is the intercept term in the variance equation. Here, ω={round(mu_garch,2)}. For any GARCH-type model, ω can be negative because the model captures volatility in logs.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nAlpha (α1): This measures the impact of lagged standardized residuals (or shocks) on the current volatility. Here, α1={round(alpha_garch,2)} describes the effect of shocks on volatility with α1 representing the magnitude effect.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nBeta (β1): This parameter indicates the persistence of volatility in the model. Here, β1={round(beta_garch,2)} describes the persistence of volatility over time.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_forecast(prov, crop, years, training_size, forecast_len):\n",
    "    # Data collection based on inputs\n",
    "    collection = data_collection(prov, crop, years)\n",
    "    # Resetting index\n",
    "    collection[\"Season\"] = collection.index\n",
    "    # Defining training and test data\n",
    "    train_size = int(len(collection) * training_size)\n",
    "    train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "\n",
    "    # Fitting the ARIMA model to the data\n",
    "    model = auto_arima(\n",
    "        train[prov],\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "    )\n",
    "\n",
    "    # Get residuals from the ARIMA model\n",
    "    residuals = model.resid()\n",
    "\n",
    "    # Fit GARCH model to the residuals\n",
    "    garch_model = arch_model(residuals, vol=\"Garch\", p=1, q=1)\n",
    "    garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "    # Fit EGARCH model\n",
    "    egarch_model = arch_model(residuals, vol=\"EGARCH\", p=1, q=1, dist=\"Normal\")\n",
    "    egarch_fit = egarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Fit TGARCH model\n",
    "    tgarch_model = arch_model(\n",
    "        residuals, vol=\"Garch\", p=1, q=1, dist=\"Normal\", power=1.0, o=1\n",
    "    )\n",
    "    tgarch_fit = tgarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Comparing models based on AIC, BIC, and log likelihood\n",
    "    models = {\"GARCH\": garch_fit, \"EGARCH\": egarch_fit, \"TGARCH\": tgarch_fit}\n",
    "\n",
    "    best_model = min(\n",
    "        models.values(), key=lambda fit: (fit.aic, fit.bic, -fit.loglikelihood)\n",
    "    )\n",
    "\n",
    "    ##Forecasting ARIMA\n",
    "    arima_forecast = model.predict(n_periods=forecast_len, return_conf_int=False)\n",
    "    # Extract all fitted parameters from the EGARCH model\n",
    "    params = best_model.params\n",
    "\n",
    "    # Initialize variables to store forecasts\n",
    "    simulated_residuals = np.zeros(forecast_len)\n",
    "    volatility_forecast = np.zeros(forecast_len)\n",
    "\n",
    "    # Check if 'resid' and 'conditional_volatility' are non-empty\n",
    "    if not best_model.resid.empty and not best_model.conditional_volatility.empty:\n",
    "        # Get the last fitted residual and variance (volatility) safely using iloc\n",
    "        last_resid = best_model.resid.iloc[-1]\n",
    "        last_variance = best_model.conditional_volatility.iloc[-1] ** 2\n",
    "\n",
    "        # Simulate the next 'forecast_horizon' steps\n",
    "        for t in range(forecast_len):\n",
    "            # Calculate the next volatility using the EGARCH model formula\n",
    "            variance_t = np.exp(\n",
    "                params[\"omega\"]\n",
    "                + params[\"alpha[1]\"]\n",
    "                * (np.abs(last_resid) / np.sqrt(last_variance) - np.sqrt(2 / np.pi))\n",
    "                + params[\"beta[1]\"] * np.log(last_variance)\n",
    "            )\n",
    "            volatility_forecast[t] = np.sqrt(variance_t)\n",
    "\n",
    "            # Simulate the next residual using the forecasted volatility\n",
    "            simulated_residuals[t] = np.random.normal(0, volatility_forecast[t])\n",
    "\n",
    "            # Update last_resid and last_variance for the next iteration\n",
    "            last_resid = simulated_residuals[t]\n",
    "            last_variance = variance_t\n",
    "\n",
    "        # Step 3: Combine ARIMA Forecasts and Simulated EGARCH Residuals\n",
    "        final_forecast = arima_forecast + simulated_residuals\n",
    "\n",
    "        # Return the ARIMA forecast and the combined ARIMA + EGARCH forecast\n",
    "        return arima_forecast, final_forecast\n",
    "\n",
    "        # Display the final forecast\n",
    "        # print(f\"ARIMA Forecast:\", arima_forecast)\n",
    "        # print(f\"Simulated {best_model.model.volatility.__class__.__name__} Residuals:\", simulated_residuals)\n",
    "        # print(f\"Final Yield Forecast (ARIMA + {best_model.model.volatility.__class__.__name__}):\", final_forecast)\n",
    "    else:\n",
    "        print(\n",
    "            f\"GARCH fit residuals or conditional volatility series are empty. Check the model fitting process.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_data(data):\n",
    "    # Convert list to DataFrame if needed\n",
    "    if isinstance(data, list):\n",
    "        data = pd.DataFrame(data, columns=[\"Values\"])\n",
    "\n",
    "    # Remove rows with 0 values\n",
    "    data = data[data[\"Values\"] != 0]\n",
    "\n",
    "    # Initialize the result dictionary\n",
    "    result = {\"Slope\": [], \"Intercept\": [], \"Predicted Value\": []}\n",
    "\n",
    "    def perform_regression(data, period):\n",
    "        X = np.arange(period).reshape(-1, 1)\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, data)\n",
    "        slope = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        predicted_value = model.predict([[period]])[0]\n",
    "        return slope, intercept, predicted_value\n",
    "\n",
    "    # Assuming 'Values' as the only column to process\n",
    "    for column in data.columns:\n",
    "        column_data = data[column].values\n",
    "        years = len(column_data)\n",
    "        slope, intercept, predicted_value = perform_regression(column_data, years)\n",
    "        result[\"Slope\"].append(slope)\n",
    "        result[\"Intercept\"].append(intercept)\n",
    "        result[\"Predicted Value\"].append(predicted_value)\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_forecast(prov, crop, years, training_size, forecast_len):\n",
    "    # Data collection based on inputs\n",
    "    collection = data_collection(prov, crop, years)\n",
    "    # Resetting index\n",
    "    collection[\"Season\"] = collection.index\n",
    "    # Defining training and test data\n",
    "    train_size = int(len(collection) * training_size)\n",
    "    train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "\n",
    "    # Fitting the ARIMA model to the data\n",
    "    model = auto_arima(\n",
    "        train[prov],\n",
    "        seasonal=False,\n",
    "        trace=True,\n",
    "        error_action=\"ignore\",\n",
    "        suppress_warnings=True,\n",
    "        stepwise=True,\n",
    "    )\n",
    "\n",
    "    # Get residuals from the ARIMA model\n",
    "    residuals = model.resid()\n",
    "\n",
    "    # Fit GARCH model to the residuals\n",
    "    garch_model = arch_model(residuals, vol=\"Garch\", p=1, q=1)\n",
    "    garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "    # Fit EGARCH model\n",
    "    egarch_model = arch_model(residuals, vol=\"EGARCH\", p=1, q=1, dist=\"Normal\")\n",
    "    egarch_fit = egarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Fit TGARCH model\n",
    "    tgarch_model = arch_model(\n",
    "        residuals, vol=\"Garch\", p=1, q=1, dist=\"Normal\", power=1.0, o=1\n",
    "    )\n",
    "    tgarch_fit = tgarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "    # Comparing models based on AIC, BIC, and log likelihood\n",
    "    models = {\"GARCH\": garch_fit, \"EGARCH\": egarch_fit, \"TGARCH\": tgarch_fit}\n",
    "\n",
    "    best_model = min(\n",
    "        models.values(), key=lambda fit: (fit.aic, fit.bic, -fit.loglikelihood)\n",
    "    )\n",
    "\n",
    "    ##Forecasting ARIMA\n",
    "    arima_forecast = model.predict(n_periods=forecast_len, return_conf_int=False)\n",
    "    # Extract all fitted parameters from the EGARCH model\n",
    "    params = best_model.params\n",
    "\n",
    "    # Initialize variables to store forecasts\n",
    "    simulated_residuals = np.zeros(forecast_len)\n",
    "    volatility_forecast = np.zeros(forecast_len)\n",
    "\n",
    "    # Check if 'resid' and 'conditional_volatility' are non-empty\n",
    "    if not best_model.resid.empty and not best_model.conditional_volatility.empty:\n",
    "        # Get the last fitted residual and variance (volatility) safely using iloc\n",
    "        last_resid = best_model.resid.iloc[-1]\n",
    "        last_variance = best_model.conditional_volatility.iloc[-1] ** 2\n",
    "\n",
    "        # Simulate the next 'forecast_horizon' steps\n",
    "        for t in range(forecast_len):\n",
    "            # Calculate the next volatility using the EGARCH model formula\n",
    "            variance_t = np.exp(\n",
    "                params[\"omega\"]\n",
    "                + params[\"alpha[1]\"]\n",
    "                * (np.abs(last_resid) / np.sqrt(last_variance) - np.sqrt(2 / np.pi))\n",
    "                + params[\"beta[1]\"] * np.log(last_variance)\n",
    "            )\n",
    "            volatility_forecast[t] = np.sqrt(variance_t)\n",
    "\n",
    "            # Simulate the next residual using the forecasted volatility\n",
    "            simulated_residuals[t] = np.random.normal(0, volatility_forecast[t])\n",
    "\n",
    "            # Update last_resid and last_variance for the next iteration\n",
    "            last_resid = simulated_residuals[t]\n",
    "            last_variance = variance_t\n",
    "\n",
    "        # Step 3: Combine ARIMA Forecasts and Simulated EGARCH Residuals\n",
    "        final_forecast = arima_forecast + simulated_residuals\n",
    "\n",
    "        # Return the ARIMA forecast and the combined ARIMA + EGARCH forecast\n",
    "        return arima_forecast, final_forecast\n",
    "\n",
    "        # Display the final forecast\n",
    "        # print(f\"ARIMA Forecast:\", arima_forecast)\n",
    "        # print(f\"Simulated {best_model.model.volatility.__class__.__name__} Residuals:\", simulated_residuals)\n",
    "        # print(f\"Final Yield Forecast (ARIMA + {best_model.model.volatility.__class__.__name__}):\", final_forecast)\n",
    "    else:\n",
    "        print(\n",
    "            f\"GARCH fit residuals or conditional volatility series are empty. Check the model fitting process.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_consistancy(prov, crop, hist_yield, initial_est, years, training_size):\n",
    "    # Collect the data\n",
    "    data = data_collection(prov, crop, years)\n",
    "    # Define regression analysis to store slope and predicted value over all years\n",
    "    reg = yield_regression_analysis(prov, crop, years)\n",
    "    reg_slope = reg[reg[\"Data Level\"] == prov][\"Slope\"].iloc[0]\n",
    "    reg_pred = reg[reg[\"Data Level\"] == prov][\"Predicted Value\"].iloc[0]\n",
    "    # Define regression analysis to store slope and predicted value over last 5 years\n",
    "    reg_short = yield_regression_analysis(prov, crop, 5)\n",
    "    reg_slope_short = reg_short[reg_short[\"Data Level\"] == prov][\"Slope\"].iloc[0]\n",
    "    reg_pred_short = reg_short[reg_short[\"Data Level\"] == prov][\"Predicted Value\"].iloc[\n",
    "        0\n",
    "    ]\n",
    "    # Historic yield data\n",
    "    if not hist_yield:\n",
    "        yc_3 = 0\n",
    "        hist_yield = (\n",
    "            \"No farm level data warrents industry data considered. Hence, industry\"\n",
    "        )\n",
    "        hist_slope = reg_slope_short\n",
    "        hist_pred = reg_pred_short\n",
    "    else:\n",
    "        reg_data_hist = reg_data(hist_yield)\n",
    "        hist_slope = round(reg_data_hist[\"Slope\"].iloc[0], 2)\n",
    "        hist_pred = round(reg_data_hist[\"Predicted Value\"].iloc[0], 2)\n",
    "\n",
    "    # ARIMA-GARCH fitting\n",
    "    def arima_forecast(prov, crop, years, training_size, forecast_len):\n",
    "        # Data collection based on inputs\n",
    "        collection = data_collection(prov, crop, years)\n",
    "        # Resetting index\n",
    "        collection[\"Season\"] = collection.index\n",
    "        # Defining training and test data\n",
    "        train_size = int(len(collection) * training_size)\n",
    "        train, test = collection.iloc[:train_size], collection.iloc[train_size:]\n",
    "\n",
    "        # Fitting the ARIMA model to the data\n",
    "        model = auto_arima(\n",
    "            train[prov],\n",
    "            seasonal=False,\n",
    "            trace=True,\n",
    "            error_action=\"ignore\",\n",
    "            suppress_warnings=True,\n",
    "            stepwise=True,\n",
    "        )\n",
    "\n",
    "        # Get residuals from the ARIMA model\n",
    "        residuals = model.resid()\n",
    "\n",
    "        # Fit GARCH model to the residuals\n",
    "        garch_model = arch_model(residuals, vol=\"Garch\", p=1, q=1)\n",
    "        garch_fit = garch_model.fit(disp=\"off\")\n",
    "\n",
    "        # Fit EGARCH model\n",
    "        egarch_model = arch_model(residuals, vol=\"EGARCH\", p=1, q=1, dist=\"Normal\")\n",
    "        egarch_fit = egarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "        # Fit TGARCH model\n",
    "        tgarch_model = arch_model(\n",
    "            residuals, vol=\"Garch\", p=1, q=1, dist=\"Normal\", power=1.0, o=1\n",
    "        )\n",
    "        tgarch_fit = tgarch_model.fit(disp=\"off\", update_freq=5)\n",
    "\n",
    "        # Comparing models based on AIC, BIC, and log likelihood\n",
    "        models = {\"GARCH\": garch_fit, \"EGARCH\": egarch_fit, \"TGARCH\": tgarch_fit}\n",
    "\n",
    "        best_model = min(\n",
    "            models.values(), key=lambda fit: (fit.aic, fit.bic, -fit.loglikelihood)\n",
    "        )\n",
    "\n",
    "        ##Forecasting ARIMA\n",
    "        arima_forecast = model.predict(n_periods=forecast_len, return_conf_int=False)\n",
    "        # Extract all fitted parameters from the EGARCH model\n",
    "        params = best_model.params\n",
    "\n",
    "        # Initialize variables to store forecasts\n",
    "        simulated_residuals = np.zeros(forecast_len)\n",
    "        volatility_forecast = np.zeros(forecast_len)\n",
    "\n",
    "        # Check if 'resid' and 'conditional_volatility' are non-empty\n",
    "        if not best_model.resid.empty and not best_model.conditional_volatility.empty:\n",
    "            # Get the last fitted residual and variance (volatility) safely using iloc\n",
    "            last_resid = best_model.resid.iloc[-1]\n",
    "            last_variance = best_model.conditional_volatility.iloc[-1] ** 2\n",
    "\n",
    "            # Simulate the next 'forecast_horizon' steps\n",
    "            for t in range(forecast_len):\n",
    "                # Calculate the next volatility using the EGARCH model formula\n",
    "                variance_t = np.exp(\n",
    "                    params[\"omega\"]\n",
    "                    + params[\"alpha[1]\"]\n",
    "                    * (np.abs(last_resid) / np.sqrt(last_variance) - np.sqrt(2 / np.pi))\n",
    "                    + params[\"beta[1]\"] * np.log(last_variance)\n",
    "                )\n",
    "                volatility_forecast[t] = np.sqrt(variance_t)\n",
    "\n",
    "                # Simulate the next residual using the forecasted volatility\n",
    "                simulated_residuals[t] = np.random.normal(0, volatility_forecast[t])\n",
    "\n",
    "                # Update last_resid and last_variance for the next iteration\n",
    "                last_resid = simulated_residuals[t]\n",
    "                last_variance = variance_t\n",
    "\n",
    "            # Step 3: Combine ARIMA Forecasts and Simulated EGARCH Residuals\n",
    "            final_forecast = arima_forecast + simulated_residuals\n",
    "\n",
    "            # Return the ARIMA forecast and the combined ARIMA + EGARCH forecast\n",
    "            return arima_forecast, final_forecast\n",
    "\n",
    "            # Display the final forecast\n",
    "            # print(f\"ARIMA Forecast:\", arima_forecast)\n",
    "            # print(f\"Simulated {best_model.model.volatility.__class__.__name__} Residuals:\", simulated_residuals)\n",
    "            # print(f\"Final Yield Forecast (ARIMA + {best_model.model.volatility.__class__.__name__}):\", final_forecast)\n",
    "        else:\n",
    "            print(\n",
    "                f\"GARCH fit residuals or conditional volatility series are empty. Check the model fitting process.\"\n",
    "            )\n",
    "\n",
    "    arm_gar, forcast_gar = arima_forecast(\n",
    "        prov, crop, years, training_size, forecast_len=7\n",
    "    )\n",
    "    arima_forecast = arm_gar.iloc[-1]\n",
    "    arima_gar_forecast = forcast_gar.iloc[-1]\n",
    "    # Defining range for forecast\n",
    "    dry_land_min = min(dry_yield_estimates[crop])\n",
    "    dry_land_max = max(dry_yield_estimates[crop])\n",
    "    min_yield = min(\n",
    "        arima_forecast,\n",
    "        arima_gar_forecast,\n",
    "        reg_pred,\n",
    "        reg_pred_short,\n",
    "        hist_pred,\n",
    "        dry_land_min,\n",
    "    )\n",
    "    max_yield = max(\n",
    "        arima_forecast,\n",
    "        arima_gar_forecast,\n",
    "        reg_pred,\n",
    "        reg_pred_short,\n",
    "        hist_pred,\n",
    "        dry_land_max,\n",
    "    )\n",
    "    med_yield = median(\n",
    "        [arima_forecast, arima_gar_forecast, reg_pred, reg_pred_short, hist_pred]\n",
    "    )\n",
    "    # Limit 10% of industry standard either side\n",
    "    yield_lim = 0.1\n",
    "    min_lim = min_yield / (1 + yield_lim)\n",
    "    max_lim = max_yield * (1 + yield_lim)\n",
    "\n",
    "    # Logic for acceotable YC calc:\n",
    "    if initial_est < min_lim or initial_est > max_lim:\n",
    "        print(\n",
    "            f\"Initial Yield Estimate exceeds acceptable bounds of dryland PCS for {crop} in {prov}. Re-evaluate your estimate.\"\n",
    "        )\n",
    "    else:\n",
    "        # Acceptable IYE\n",
    "        # Logic to assign points for yc_1 - yield estimates\n",
    "        if initial_est <= med_yield:\n",
    "            if initial_est > min_yield:\n",
    "                yc_1 = round(25 * (initial_est / med_yield), 0)\n",
    "            elif initial_est > min_lim:\n",
    "                yc_1 = round(20 * (initial_est / min_yield), 0)\n",
    "            else:\n",
    "                yc_1 = 0\n",
    "        else:\n",
    "            if initial_est <= max_yield:\n",
    "                m = -5 / (max_yield - med_yield)\n",
    "                c = 20 - m * max_yield\n",
    "                yc_1 = round(m * initial_est + c, 0)\n",
    "            elif initial_est <= max_lim and initial_est > max_yield:\n",
    "                m = -20 / (max_lim - max_yield)\n",
    "                c = 20 - m * max_yield\n",
    "                yc_1 = round(m * initial_est + c, 0)\n",
    "            else:\n",
    "                yc_1 = 0\n",
    "\n",
    "        # Logic for trend analysis points yc_2, confirm with business\n",
    "\n",
    "        if reg_slope > -0.25:\n",
    "            if reg_slope_short < -0.25:\n",
    "                yc_2 = 1\n",
    "            elif reg_slope_short > 0:\n",
    "                yc_2 = 5\n",
    "            else:\n",
    "                yc_2 = 3\n",
    "        else:\n",
    "            if reg_slope_short < -0.25:\n",
    "                yc_2 = 0\n",
    "            elif reg_slope_short > 0.25:\n",
    "                yc_2 = 4\n",
    "            else:\n",
    "                yc_2 = 2\n",
    "\n",
    "        # Logic for farmer historic yield, confirm with business\n",
    "        if not hist_yield:\n",
    "            yc_3 = 0\n",
    "        else:\n",
    "            if hist_slope < -0.25:\n",
    "                yc_3 = 0\n",
    "            elif hist_slope < 0.25:\n",
    "                yc_3 = 2\n",
    "            else:\n",
    "                yc_3 = 4\n",
    "\n",
    "        # Final Yield Consistancy score\n",
    "        yc = yc_1 + yc_2 + yc_3\n",
    "\n",
    "        print(f\"For a farmer based in {prov} planning to grow {crop}:\")\n",
    "        print(f\"A Yield Consistancy (YC) of {yc} out of a possible 35 points.\")\n",
    "        print(\n",
    "            f\"For an initial yield estimate of {initial_est} t/ha compared to area min {round(min_yield,2)} t/ha, max {round(max_yield,2)} t/ha. AYE = {round(med_yield,2)} t/ha with limits of {round(min_lim,2)} t/ha and {round(max_lim,2)} t/ha.\"\n",
    "        )\n",
    "        print(f\"Industry benchmarks define a {yc_1} from a possible 25 points.\")\n",
    "        print(\n",
    "            f\"5 Year Historical Industry trend analysis sugests a predicted yield of {round(reg_pred_short,2)} t/ha and a slope of {round(reg_slope_short,2)} t/ha yields {yc_2} from a possible 5 points.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Historical farm data trend analysis: {hist_yield} sugests a predicted yield of {round(hist_pred,2)} t/ha and a slope of {round(hist_slope,2)} t/ha yields {yc_3} from a possible 5 points.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.20 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=76.188, Time=0.03 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=71.943, Time=0.04 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.13 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=74.996, Time=0.02 sec\n",
      " ARIMA(2,1,0)(0,0,0)[0] intercept   : AIC=73.898, Time=0.05 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.17 sec\n",
      " ARIMA(2,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.34 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0]             : AIC=72.874, Time=0.05 sec\n",
      "\n",
      "Best model:  ARIMA(1,1,0)(0,0,0)[0] intercept\n",
      "Total fit time: 1.033 seconds\n",
      "For a farmer based in Eastern Cape planning to grow white maize:\n",
      "A Yield Consistancy (YC) of 34.0 out of a possible 35 points.\n",
      "For an initial yield estimate of 7.21 t/ha compared to area min 2.5 t/ha, max 8.52 t/ha. AYE = 7.23 t/ha with limits of 2.27 t/ha and 9.37 t/ha.\n",
      "Industry benchmarks define a 25.0 from a possible 25 points.\n",
      "5 Year Historical Industry trend analysis sugests a predicted yield of 4.22 t/ha and a slope of 0.69 t/ha yields 5 from a possible 5 points.\n",
      "Historical farm data trend analysis: No farm level data warrents industry data considered. Hence, industry sugests a predicted yield of 4.22 t/ha and a slope of 0.69 t/ha yields 4 from a possible 5 points.\n"
     ]
    }
   ],
   "source": [
    "yield_consistancy(\n",
    "    prov=\"Eastern Cape\",\n",
    "    crop=\"white maize\",\n",
    "    hist_yield=[],\n",
    "    initial_est=7.21,\n",
    "    years=34,\n",
    "    training_size=0.85,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
